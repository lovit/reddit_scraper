{
  "title": "[P] ML Alternatives For Causal Inference To Pandas Groupby For High Dimensional Data",
  "comments": [
    {
      "author_fullname": "t2_32eef",
      "created_utc": 1547357648.0,
      "score": 2,
      "body_html": "<div class=\"md\"><p>Is the 20 dimensional data the &quot;high dimensional&quot; data you are talking about? </p>\n</div>",
      "body": "Is the 20 dimensional data the \"high dimensional\" data you are talking about? ",
      "id": "edydlm4"
    },
    {
      "author_fullname": "t2_kgihw",
      "created_utc": 1547401302.0,
      "score": 1,
      "body_html": "<div class=\"md\"><p>Why are you using a neural network and not something like a random forest or gradient boosting? It sounds like you’ve got tabular data. Neural networks aren’t typically the best for tabular data.</p>\n\n<p>Probably: forget about neural nets. There’s nothing special about a neural net unless you’ve got a very problem-specific architecture. If you’re using a vanilla feed forward NN, you’re definitely wasting your time.</p>\n\n<p>Causal inference is something that you can’t really get from standard ML.</p>\n\n<p>You can definitely get “generalizable correlation”; meaning that you might find it in a subset of data and then find that the same relationship still holds out of sample.</p>\n\n<p>The first thing I’d do is univariate models with e.g. a random forest. Which single variable creates the best model? If any really stick out, investigate them. </p>\n\n<p>The next is to ablate the variables. Eliminate them one at a time using lowest feature importance. Take a look at the error. Can you get something close to the same error you get with all features by using e.g. just 3? If so, start taking a look at these 3.</p>\n\n<p>Once you have perhaps 3, try building some shallow decision trees and break the data into maybe 4-8 cases (nodes) and see what they look like. Do the splits seem meaningful?</p>\n\n<p>If you have 2 features, try making a heatmap w the target as the color. Do you see a pattern? Investigate it.</p>\n\n<p>Modeling can only tell you about correlation. If you want to investigate causation, you can grab a hypothesis from your features / current dataset, and construct a new dataset where you control the features in your hypothesis. </p>\n\n<p>If you want to see whether you can exert control on a system, you can only be sure by trying to exert control and seeing what happens. </p>\n</div>",
      "body": "Why are you using a neural network and not something like a random forest or gradient boosting? It sounds like you’ve got tabular data. Neural networks aren’t typically the best for tabular data.\n\nProbably: forget about neural nets. There’s nothing special about a neural net unless you’ve got a very problem-specific architecture. If you’re using a vanilla feed forward NN, you’re definitely wasting your time.\n\nCausal inference is something that you can’t really get from standard ML.\n\nYou can definitely get “generalizable correlation”; meaning that you might find it in a subset of data and then find that the same relationship still holds out of sample.\n\nThe first thing I’d do is univariate models with e.g. a random forest. Which single variable creates the best model? If any really stick out, investigate them. \n\nThe next is to ablate the variables. Eliminate them one at a time using lowest feature importance. Take a look at the error. Can you get something close to the same error you get with all features by using e.g. just 3? If so, start taking a look at these 3.\n\nOnce you have perhaps 3, try building some shallow decision trees and break the data into maybe 4-8 cases (nodes) and see what they look like. Do the splits seem meaningful?\n\nIf you have 2 features, try making a heatmap w the target as the color. Do you see a pattern? Investigate it.\n\nModeling can only tell you about correlation. If you want to investigate causation, you can grab a hypothesis from your features / current dataset, and construct a new dataset where you control the features in your hypothesis. \n\nIf you want to see whether you can exert control on a system, you can only be sure by trying to exert control and seeing what happens. ",
      "id": "edzkha5"
    },
    {
      "author_fullname": "t2_6atxw",
      "created_utc": 1547408535.0,
      "score": 1,
      "body_html": "<div class=\"md\"><p>Are you familiar with causal inference? There&#39;s a <a href=\"https://github.com/Microsoft/dowhy\">python library</a> maintained by a small group at MS that has implemented do-calculus for causal effect estimation if that&#39;s what you&#39;re after. It&#39;s not clear from the prompt whether you&#39;re asking how to think about estimating causal effects or if you&#39;re asking for implementations of methods you&#39;re familiar with already. </p>\n</div>",
      "body": "Are you familiar with causal inference? There's a [python library](https://github.com/Microsoft/dowhy) maintained by a small group at MS that has implemented do-calculus for causal effect estimation if that's what you're after. It's not clear from the prompt whether you're asking how to think about estimating causal effects or if you're asking for implementations of methods you're familiar with already. ",
      "id": "edzw812"
    }
  ],
  "created_utc": 1547348586.0,
  "author_fullname": "t2_2ze3p1tc",
  "selftext": "Hello Experts!\n\ni have data where each row represents a single session of traffic. There are many different features - the source and destination IPs, application, ports, time, etc., Each row also has metrics for that session that have been gathered by a special calculation process that runs on the device the traffic is passing through. There are many tens of thousands of observations.\n\nOr speaking high level, we already know how each session is performing, and a lot about it - and we have a lot of observations.\n\ni have code that massages all the data into bins, vectors, does normalization, encoding, etc., - and i can feed it into a neural net and see the loss go way down as the neural net learns how to predict what i already know. But i am not really looking at making a prediction. i am looking for cause.\n\ni am just curious how successful anyone has been with doing causal inference. i seem to be able to make general statements about the mean/median performance values by doing some fancy pandas groupby work - but that can be fairly labor intensive, and is limited to only the combinations i think of.\n\nThere are maybe 20 features or so. And i usually have vectors with something like 40 values max, after i encode the categorical variables. i've tried to avoid going higher than that by mapping low frequency labels to a single \"Rare\" identifier.\n\nDoes anyone have any thoughts, or especially code/module recommendations that would be helpful in high dimensional data?\n\nThanks very much in advance.",
  "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hello Experts!</p>\n\n<p>i have data where each row represents a single session of traffic. There are many different features - the source and destination IPs, application, ports, time, etc., Each row also has metrics for that session that have been gathered by a special calculation process that runs on the device the traffic is passing through. There are many tens of thousands of observations.</p>\n\n<p>Or speaking high level, we already know how each session is performing, and a lot about it - and we have a lot of observations.</p>\n\n<p>i have code that massages all the data into bins, vectors, does normalization, encoding, etc., - and i can feed it into a neural net and see the loss go way down as the neural net learns how to predict what i already know. But i am not really looking at making a prediction. i am looking for cause.</p>\n\n<p>i am just curious how successful anyone has been with doing causal inference. i seem to be able to make general statements about the mean/median performance values by doing some fancy pandas groupby work - but that can be fairly labor intensive, and is limited to only the combinations i think of.</p>\n\n<p>There are maybe 20 features or so. And i usually have vectors with something like 40 values max, after i encode the categorical variables. i&#39;ve tried to avoid going higher than that by mapping low frequency labels to a single &quot;Rare&quot; identifier.</p>\n\n<p>Does anyone have any thoughts, or especially code/module recommendations that would be helpful in high dimensional data?</p>\n\n<p>Thanks very much in advance.</p>\n</div><!-- SC_ON -->",
  "id": "aff20r"
}